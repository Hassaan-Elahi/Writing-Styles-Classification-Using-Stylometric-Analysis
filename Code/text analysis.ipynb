{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa0cce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in d:\\programdata\\anaconda3\\lib\\site-packages (0.8)\n"
     ]
    }
   ],
   "source": [
    "# Text Analysis code from Hassaan-Elahi \n",
    "# https://github.com/Hassaan-Elahi/Writing-Styles-Classification-Using-Stylometric-Analysis\n",
    "#\n",
    "# Ported to this Jupiter notebook by AeonLabs Solutions\n",
    "#    https://github.com/aeonSolutions/AeonLabs--Writing-Styles-Classification-Using-Stylometric-Analysis\n",
    "#    Jupiter notebook code change log:\n",
    "#       - Running on a X64 Windows Mahine using Anaconda\n",
    "#       - Notebook loads a MS Word DocX file, converts it to txt, and do Writing style analysis from Hassaan-Elahi \n",
    "\n",
    "import collections as coll\n",
    "import math\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import style\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "!pip install docx2txt\n",
    "import docx2txt\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8244bdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Aeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "cmuDictionary = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab3e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a paragraph of text and divides it into chunks of specified number of sentences\n",
    "def slidingWindow(sequence, winSize, step=1):\n",
    "    try:\n",
    "        it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    "\n",
    "    sequence = sent_tokenize(sequence)\n",
    "\n",
    "    # Pre-compute number of chunks to omit\n",
    "    numOfChunks = int(((len(sequence) - winSize) / step) + 1)\n",
    "\n",
    "    l = []\n",
    "    # Do the work\n",
    "    for i in range(0, numOfChunks * step, step):\n",
    "        l.append(\" \".join(sequence[i:i + winSize]))\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def syllable_count_Manual(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"e\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# COUNTS NUMBER OF SYLLABLES\n",
    "\n",
    "def syllable_count(word):\n",
    "    global cmuDictionary\n",
    "    d = cmuDictionary\n",
    "    try:\n",
    "        syl = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except:\n",
    "        syl = syllable_count_Manual(word)\n",
    "    return syl\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# removing stop words plus punctuation.\n",
    "def Avg_wordLength(str):\n",
    "    str.translate(string.punctuation)\n",
    "    tokens = word_tokenize(str, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    return np.average([len(word) for word in words])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# returns avg number of characters in a sentence\n",
    "def Avg_SentLenghtByCh(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token) for token in tokens])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# returns avg number of words in a sentence\n",
    "def Avg_SentLenghtByWord(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token.split()) for token in tokens])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# GIVES NUMBER OF SYLLABLES PER WORD\n",
    "def Avg_Syllable_per_Word(text):\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    syllabls = [syllable_count(word) for word in words]\n",
    "    p = (\" \".join(words))\n",
    "    return sum(syllabls) / max(1, len(words))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# COUNTS SPECIAL CHARACTERS NORMALIZED OVER LENGTH OF CHUNK\n",
    "def CountSpecialCharacter(text):\n",
    "    st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return count / len(text)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def CountPuncuation(text):\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return float(count) / float(len(text))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# RETURNS NORMALIZED COUNT OF FUNCTIONAL WORDS FROM A Framework for\n",
    "# Authorship Identification of Online Messages: Writing-Style Features and Classification Techniques\n",
    "\n",
    "def CountFunctionalWords(text):\n",
    "    functional_words = \"\"\"a between in nor some upon\n",
    "    about both including nothing somebody us\n",
    "    above but inside of someone used\n",
    "    after by into off something via\n",
    "    all can is on such we\n",
    "    although cos it once than what\n",
    "    am do its one that whatever\n",
    "    among down latter onto the when\n",
    "    an each less opposite their where\n",
    "    and either like or them whether\n",
    "    another enough little our these which\n",
    "    any every lots outside they while\n",
    "    anybody everybody many over this who\n",
    "    anyone everyone me own those whoever\n",
    "    anything everything more past though whom\n",
    "    are few most per through whose\n",
    "    around following much plenty till will\n",
    "    as for must plus to with\n",
    "    at from my regarding toward within\n",
    "    be have near same towards without\n",
    "    because he need several under worth\n",
    "    before her neither she unless would\n",
    "    behind him no should unlike yes\n",
    "    below i nobody since until you\n",
    "    beside if none so up your\n",
    "    \"\"\"\n",
    "\n",
    "    functional_words = functional_words.split()\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    count = 0\n",
    "\n",
    "    for i in text:\n",
    "        if i in functional_words:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(words)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# also returns Honore Measure R\n",
    "def hapaxLegemena(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    V1 = 0\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words:\n",
    "        freqs[word] += 1\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 1:\n",
    "            V1 += 1\n",
    "    N = len(words)\n",
    "    V = float(len(set(words)))\n",
    "    R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
    "    h = V1 / N\n",
    "    return R, h\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def hapaxDisLegemena(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    count = 0\n",
    "    # Collections as coll Counter takes an iterable collapse duplicate and counts as\n",
    "    # a dictionary how many equivelant items has been entered\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 2:\n",
    "            count += 1\n",
    "\n",
    "    h = count / float(len(words))\n",
    "    S = count / float(len(set(words)))\n",
    "    return S, h\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# c(w)  = ceil (log2 (f(w*)/f(w))) f(w*) frequency of most commonly used words f(w) frequency of word w\n",
    "# measure of vocabulary richness and connected to zipfs law, f(w*) const rak kay zips law say rank nikal rahay hein\n",
    "def AvgWordFrequencyClass(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words:\n",
    "        freqs[word] += 1\n",
    "    maximum = float(max(list(freqs.values())))\n",
    "    return np.average([math.floor(math.log((maximum + 1) / (freqs[word]) + 1, 2)) for word in words])\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# TYPE TOKEN RATIO NO OF DIFFERENT WORDS / NO OF WORDS\n",
    "def typeTokenRatio(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# logW = V-a/log(N)\n",
    "# N = total words , V = vocabulary richness (unique words) ,  a=0.17\n",
    "# we can convert into log because we are only comparing different texts\n",
    "def BrunetsMeasureW(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    a = 0.17\n",
    "    V = float(len(set(words)))\n",
    "    N = len(words)\n",
    "    B = (V - a) / (math.log(N))\n",
    "    return B\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "def RemoveSpecialCHs(text):\n",
    "    text = word_tokenize(text)\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "\n",
    "    words = [word for word in text if word not in st]\n",
    "    return words\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# K  10,000 * (M - N) / N**2\n",
    "# , where M  Sigma i**2 * Vi.\n",
    "def YulesCharacteristicK(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    N = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    vi = coll.Counter()\n",
    "    vi.update(freqs.values())\n",
    "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
    "    K = 10000 * (M - N) / math.pow(N, 2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -1*sigma(pi*lnpi)\n",
    "# Shannon and sympsons index are basically diversity indices for any community\n",
    "def ShannonEntropy(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    lenght = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    arr = np.array(list(freqs.values()))\n",
    "    distribution = 1. * arr\n",
    "    distribution /= max(1, lenght)\n",
    "    import scipy as sc\n",
    "    H = sc.stats.entropy(distribution, base=2)\n",
    "    # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
    "    return H\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1 - (sigma(n(n - 1))/N(N-1)\n",
    "# N is total number of words\n",
    "# n is the number of each type of word\n",
    "def SimpsonsIndex(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    N = len(words)\n",
    "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "    D = 1 - (n / (N * (N - 1)))\n",
    "    return D\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def FleschReadingEase(text, NoOfsentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    l = float(len(words))\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    I = 206.835 - 1.015 * (l / float(NoOfsentences)) - 84.6 * (scount / float(l))\n",
    "    return I\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "def FleschCincadeGradeLevel(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    l = len(words)\n",
    "    F = 0.39 * (l / NoOfSentences) + 11.8 * (scount / float(l)) - 15.59\n",
    "    return F\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "def dale_chall_readability_formula(text, NoOfSectences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    difficult = 0\n",
    "    adjusted = 0\n",
    "    NoOfWords = len(words)\n",
    "    with open('dale-chall.pkl', 'rb') as f:\n",
    "        fimiliarWords = pickle.load(f)\n",
    "    for word in words:\n",
    "        if word not in fimiliarWords:\n",
    "            difficult += 1\n",
    "    percent = (difficult / NoOfWords) * 100\n",
    "    if (percent > 5):\n",
    "        adjusted = 3.6365\n",
    "    D = 0.1579 * (percent) + 0.0496 * (NoOfWords / NoOfSectences) + adjusted\n",
    "    return D\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def GunningFoxIndex(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    NoOFWords = float(len(words))\n",
    "    complexWords = 0\n",
    "    for word in words:\n",
    "        if (syllable_count(word) > 2):\n",
    "            complexWords += 1\n",
    "\n",
    "    G = 0.4 * ((NoOFWords / NoOfSentences) + 100 * (complexWords / NoOFWords))\n",
    "    return G\n",
    "\n",
    "\n",
    "def PrepareData(text1, text2, Winsize):\n",
    "    chunks1 = slidingWindow(text1, Winsize, Winsize)\n",
    "    chunks2 = slidingWindow(text2, Winsize, Winsize)\n",
    "    return \" \".join(str(chunk1) + str(chunk2) for chunk1, chunk2 in zip(chunks1, chunks2))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# returns a feature vector of text\n",
    "def FeatureExtration(text, winSize, step):\n",
    "    # cmu dictionary for syllables\n",
    "    global cmuDictionary\n",
    "    cmuDictionary = cmudict.dict()\n",
    "\n",
    "    chunks = slidingWindow(text, winSize, step)\n",
    "    vector = []\n",
    "    for chunk in chunks:\n",
    "        feature = []\n",
    "\n",
    "        # LEXICAL FEATURES\n",
    "        meanwl = (Avg_wordLength(chunk))\n",
    "        feature.append(meanwl)\n",
    "\n",
    "        meansl = (Avg_SentLenghtByCh(chunk))\n",
    "        feature.append(meansl)\n",
    "\n",
    "        mean = (Avg_SentLenghtByWord(chunk))\n",
    "        feature.append(mean)\n",
    "\n",
    "        meanSyllable = Avg_Syllable_per_Word(chunk)\n",
    "        feature.append(meanSyllable)\n",
    "\n",
    "        means = CountSpecialCharacter(chunk)\n",
    "        feature.append(means)\n",
    "\n",
    "        p = CountPuncuation(chunk)\n",
    "        feature.append(p)\n",
    "\n",
    "        f = CountFunctionalWords(text)\n",
    "        feature.append(f)\n",
    "\n",
    "        # VOCABULARY RICHNESS FEATURES\n",
    "\n",
    "        TTratio = typeTokenRatio(chunk)\n",
    "        feature.append(TTratio)\n",
    "\n",
    "        HonoreMeasureR, hapax = hapaxLegemena(chunk)\n",
    "        feature.append(hapax)\n",
    "        feature.append(HonoreMeasureR)\n",
    "\n",
    "        SichelesMeasureS, dihapax = hapaxDisLegemena(chunk)\n",
    "        feature.append(dihapax)\n",
    "        feature.append(SichelesMeasureS)\n",
    "\n",
    "        YuleK = YulesCharacteristicK(chunk)\n",
    "        feature.append(YuleK)\n",
    "\n",
    "        S = SimpsonsIndex(chunk)\n",
    "        feature.append(S)\n",
    "\n",
    "        B = BrunetsMeasureW(chunk)\n",
    "        feature.append(B)\n",
    "\n",
    "        Shannon = ShannonEntropy(text)\n",
    "        feature.append(Shannon)\n",
    "\n",
    "        # READIBILTY FEATURES\n",
    "        FR = FleschReadingEase(chunk, winSize)\n",
    "        feature.append(FR)\n",
    "\n",
    "        FC = FleschCincadeGradeLevel(chunk, winSize)\n",
    "        feature.append(FC)\n",
    "\n",
    "        # also quite a different\n",
    "        D = dale_chall_readability_formula(chunk, winSize)\n",
    "        feature.append(D)\n",
    "\n",
    "        # quite a difference\n",
    "        G = GunningFoxIndex(chunk, winSize)\n",
    "        feature.append(G)\n",
    "\n",
    "        vector.append(feature)\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ELBOW METHOD\n",
    "def ElbowMethod(data):\n",
    "    X = data  # <your_data>\n",
    "    distorsions = []\n",
    "    for k in range(1, 10):\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "        distorsions.append(kmeans.inertia_)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    plt.plot(range(1, 10), distorsions, 'bo-')\n",
    "    plt.grid(True)\n",
    "    plt.ylabel(\"Square Root Error\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.title('Elbow curve')\n",
    "    plt.savefig(\"ElbowCurve.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ANALYSIS PART\n",
    "\n",
    "# Using the graph shown in Elbow Method, find the appropriate value of K and set it here.\n",
    "def Analysis(vector, K=2):\n",
    "    arr = (np.array(vector))\n",
    "\n",
    "    # mean normalization of the data . converting into normal distribution having mean=0 , -0.1<x<0.1\n",
    "    sc = StandardScaler()\n",
    "    x = sc.fit_transform(arr)\n",
    "\n",
    "    # Breaking into principle components\n",
    "    pca = PCA(n_components=2)\n",
    "    components = (pca.fit_transform(x))\n",
    "    # Applying kmeans algorithm for finding centroids\n",
    "\n",
    "    kmeans = KMeans(n_clusters=K)\n",
    "    kmeans.fit_transform(components)\n",
    "    print(\"labels: \", kmeans.labels_)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # lables are assigned by the algorithm if 2 clusters then lables would be 0 or 1\n",
    "    lables = kmeans.labels_\n",
    "    colors = [\"r.\", \"g.\", \"b.\", \"y.\", \"c.\"]\n",
    "    colors = colors[:K + 1]\n",
    "\n",
    "    for i in range(len(components)):\n",
    "        plt.plot(components[i][0], components[i][1], colors[lables[i]], markersize=10)\n",
    "\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], marker=\"x\", s=150, linewidths=10, zorder=15)\n",
    "    plt.xlabel(\"1st Principle Component\")\n",
    "    plt.ylabel(\"2nd Principle Component\")\n",
    "    title = \"Styles Clusters\"\n",
    "    plt.title(title)\n",
    "    plt.savefig(\"Results\" + \".png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8065bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Example file:\n",
    "docxFilename = 'paper_to_submit.docx'\n",
    "\n",
    "\n",
    "# Try to delete the file.\n",
    "try:\n",
    "    os.remove(docxFilename[:-5]+'.txt')\n",
    "except OSError as e:\n",
    "    # If it fails, inform the user.\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    \n",
    "\n",
    "with open(docxFilename, 'rb') as infile:\n",
    "    outfile = open(docxFilename[:-5]+'.txt', 'w', encoding='utf-8')\n",
    "    doc = docx2txt.process(infile)\n",
    "\n",
    "    outfile.write(doc)\n",
    "\n",
    "outfile.close()\n",
    "infile.close()\n",
    "\n",
    "print(\"=========\")\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try any text file here\n",
    "text = open(docxFilename[:-5]+'.txt', encoding=\"utf-8\", errors=\"ignore\", ).read()\n",
    "\n",
    "vector = FeatureExtration(text, winSize=10, step=10)\n",
    "ElbowMethod(np.array(vector))\n",
    "Analysis(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45bcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
